# Lecture 2: How AI "Thinks" - Peeking Behind the Digital Curtain

## 2.1. Welcome Back, AI Detectives!

### [seq:010] Previously on AI Detectives...

###### SCRIPT
Welcome back, everyone! Last time, we put on our detective hats and started investigating the world of AI. Who remembers what critical thinking means? That's right â€“ it's like being a detective for your brain! Today, we're going to peek behind the digital curtain and discover how AI actually "thinks." Spoiler alert: it doesn't think like we do at all!

###### VISUAL
- Split screen showing: Human brain with thought bubbles vs Computer with data streams
- Title: "How AI 'Thinks'"
- Animated recap icons from Lecture 1

###### NOTES
- Quick warm-up: Ask students to share one AI they interacted with this week
- Emphasize quotes around "thinks" - AI doesn't actually think!
- Connection to video games: AI is like NPCs following complex scripts

---

## 2.2. The AI Kitchen: Cooking Up Responses

### [seq:020] Recipe for AI Responses

###### SCRIPT
Imagine AI as a super-fast chef in a digital kitchen. Instead of ingredients like flour and eggs, it uses billions of text examples it learned from. When you ask it something, it's not "thinking" â€“ it's rapidly mixing patterns it has seen before to cook up something that looks right. Sometimes the dish is perfect, sometimes it's... interesting!

###### VISUAL
- Animated kitchen with AI chef
- Ingredients labeled: "Wikipedia texts," "News articles," "Books," "Websites"
- Mixing bowl showing pattern combinations
- Output plate with generated text

###### NOTES
- Relate to autocomplete on phones - same basic idea, just way more powerful
- AI has no real understanding, just pattern matching
- Like a parrot that's really, really good at sounding human

###### DEMONSTRATION
Try this with students:
1. Ask ChatGPT: "Explain how you work in exactly 3 sentences"
2. Ask again with same prompt
3. Notice different responses despite same input
4. Discuss: Why different each time?

---

## 2.3. When AI Hallucinates

### [seq:030] AI's Imagination Gone Wild

###### SCRIPT
Here's something wild â€“ AI can hallucinate! Not like seeing pink elephants, but it can confidently make up facts that sound totally real. It might tell you about a book that doesn't exist, or claim a historical event happened when it didn't. Why? Because it's great at making things sound believable, even when they're completely made up!

###### VISUAL
- Comic-style illustration of AI with swirly eyes
- Speech bubbles with confident but false statements
- Examples: "The Great Pyramid was built in 1823" (X mark)
- Warning sign: "Sounds right â‰  Is right"

###### NOTES
- Hallucination happens because AI predicts "likely" text, not "true" text
- Famous example: Lawyers using ChatGPT cited fake court cases
- Always verify surprising or important information

###### ACTIVITY
"Hallucination Hunt" game:
1. Groups ask AI for facts about made-up topics
2. Example: "Tell me about the Purple Tuesday of 1987"
3. Watch AI confidently explain something that never existed
4. Discuss: How can we spot hallucinations?

---

## 2.4. Training Data: You Are What You Read

### [seq:040] The AI Diet

###### SCRIPT
Just like you are what you eat, AI is what it reads! These systems learned from massive amounts of text from the internet. But here's the thing â€“ the internet has everything from Nobel Prize research to conspiracy theories. AI swallowed it all and now tries to guess what response fits best. It's like learning to cook by reading every recipe ever written, including the terrible ones!

###### VISUAL
- Pie chart showing training data sources
- Good sources (academic) vs questionable sources (random forums)
- AI "eating" different types of content
- Resulting "knowledge smoothie" with mixed quality

###### NOTES
- Training data cutoff dates - AI doesn't know recent events
- Biases in data lead to biased outputs
- Quality varies: Wikipedia vs Reddit comments

###### DEMONSTRATION
Test AI's training limits:
1. Ask about something very recent (last month)
2. Ask about something niche/specialized
3. Ask about different cultural perspectives
4. Notice where it struggles or defaults to generic answers

---

## 2.5. Pattern Matching vs Understanding

### [seq:050] The Parrot Problem

###### SCRIPT
Here's a mind-bender: AI is like the world's best parrot. It can repeat and remix everything it's learned in ways that seem intelligent. But does it understand what it's saying? Nope! It's pattern matching on steroids. When it helps with math, it's not "doing math" â€“ it's recognizing patterns from millions of math examples it's seen.

###### VISUAL
- Split comparison: Parrot repeating vs Human understanding
- Math example: AI showing work vs just pattern matching
- Visualization of pattern recognition process
- "Looks smart â‰  Is smart" message

###### NOTES
- Chinese Room thought experiment simplified for teens
- AI passes tests by pattern matching, not comprehension
- Why AI can write poetry but can't truly feel emotions

###### REFLECTION
Questions for discussion:
- If AI doesn't understand, can it truly be creative?
- What's the difference between seeming smart and being smart?
- Does it matter if AI understands, as long as it helps?

---

## 2.6. Why AI Gets Confused

### [seq:060] Lost in Translation

###### SCRIPT
Ever played a game of telephone where the message gets totally scrambled? AI can get confused in similar ways. Ask it to count letters in a word, and it might get it wrong. Why? Because it learned from text, not by actually counting. It's like trying to describe a color to someone who's never seen â€“ the AI is working with patterns, not real understanding.

###### VISUAL
- Examples of AI failing at simple tasks
- Letter counting fails: "How many R's in strawberry?"
- Logic puzzle failures
- Visual showing pattern matching vs actual reasoning

###### NOTES
- AI struggles with tasks requiring true reasoning
- Often fails at things 5-year-olds can do easily
- Confidence doesn't equal correctness

###### DEMONSTRATION
"Stump the AI" challenges:
1. Letter counting: "How many E's in 'telephone'?"
2. Simple riddles that require real-world understanding
3. Basic spatial reasoning questions
4. Discuss why these are hard for AI

---

## 2.7. Speed vs Accuracy

### [seq:070] The Fast and the Spurious

###### SCRIPT
AI is incredibly fast â€“ it can write an essay in seconds that would take you an hour. But faster isn't always better! It's like a student who raises their hand immediately for every question, sometimes getting it right, sometimes hilariously wrong. The speed is impressive, but it comes at a cost: AI doesn't double-check its work or really think things through.

###### VISUAL
- Race track with AI zooming past "Accuracy" checkpoint
- Speedometer showing "FAST" but accuracy meter wobbling
- Examples of quick but wrong answers
- Turtle (careful human) vs Hare (fast AI) remake

###### NOTES
- Speed impresses us, but accuracy matters more
- AI can't reflect on its answers
- Quick generation can mean more errors slip through

###### ACTIVITY
"Speed vs Quality" experiment:
1. Race: Students vs AI writing a short paragraph
2. Then fact-check both versions
3. Discuss: Which errors are easier to spot?
4. Lesson: Fast isn't always best

---

## 2.7.5. The Reasoning Revolution

### [seq:075] When AI Learns to "Think Out Loud"

###### SCRIPT
Plot twist: Some newer AIs are learning to "think out loud" before answering! ðŸ§ âœ¨ Models like DeepSeek-R1-0528 use something called "chain-of-thought reasoning" â€“ they show their work like you do in math class. Instead of jumping straight to an answer, they break down problems step by step. It's like watching AI's thought process in slow motion! This helps them get harder questions right, especially in math and coding. But remember â€“ they're still pattern matching, just with more elaborate patterns!

###### VISUAL
- Split screen: "Old AI" (instant answer) vs "New AI" (shows thinking steps)
- Thought bubble chain showing reasoning process
- Math problem example with step-by-step solving
- Clock showing "thinking time" vs "instant response"
- Code example: DeepSeek showing work vs other AI just giving answer

###### NOTES
- Chain-of-thought reasoning is a major 2024-2025 breakthrough
- DeepSeek R1-0528 uses ~23K tokens for complex problems (vs 12K in previous version)
- Distillation breakthrough: 8B models can now match 235B models through better reasoning
- Still pattern matching, just more sophisticated patterns
- Cost-effective: Better reasoning at lower cost than closed models

###### DEMONSTRATION
"Reasoning Comparison" live test:
1. Give complex math problem to regular AI vs reasoning-enabled AI
2. Show how DeepSeek R1-0528 "thinks through" the problem
3. Compare accuracy and explanation quality
4. Discuss: Does showing work always mean better answers?

###### LINKS
- DeepSeek R1-0528 technical report and benchmarks
- Examples of chain-of-thought vs direct answering
- AIME 2024 benchmark results (87.5% accuracy)
- Cost comparison: DeepSeek vs closed models

---

## 2.8. The Confidence Game

### [seq:080] Why AI Always Sounds So Sure

###### SCRIPT
Have you noticed AI never says "I don't know" unless programmed to? It's designed to always provide an answer, even when it should admit uncertainty. It's like that friend who confidently gives directions even when they're lost. This overconfidence is one of AI's biggest weaknesses â€“ and why we need our critical thinking skills more than ever!

###### VISUAL
- Confidence meter stuck at 100%
- AI saying wrong answers with authority
- Comparison: Human saying "I think..." vs AI stating as fact
- Warning signs for overconfident responses

###### NOTES
- AI lacks self-awareness about its limitations
- Designed to be helpful, not necessarily accurate
- Confidence is programmed, not earned

###### DEMONSTRATION
"Confidence Check" exercise:
1. Ask AI about increasingly obscure topics
2. Note how confidence doesn't decrease
3. Ask it to rate its own confidence (often says high)
4. Verify actual accuracy of obscure claims

---

## 2.9. Putting It All Together

### [seq:090] Your AI Operations Manual

###### SCRIPT
Now you know AI's secrets! It's a pattern-matching machine that learned from tons of text, works incredibly fast, but doesn't truly understand anything. It hallucinates, shows biases from its training, and sounds confident even when wrong. But knowing all this makes you powerful â€“ you can use AI as a tool while staying smarter than it!

###### VISUAL
- Infographic: "AI Operations Manual"
- Key points with icons: Pattern matching, No understanding, Super fast, Always confident
- Your toolkit: Verify, Question, Think critically
- Badge: "AI Mechanic Level 2"

###### NOTES
- Emphasize AI as tool, not authority
- Knowledge is power - now they know how it works
- Ready to spot AI mistakes in the wild

###### ACTIVITY
Create an "AI User Guide" poster:
1. Teams design warning labels for AI
2. Include: Strengths, Weaknesses, How to use safely
3. Make it creative and memorable
4. Share and vote on best guide

---

## 2.10. Homework & Next Time

### [seq:100] Mission: AI Behavior Detective

###### SCRIPT
Your mission this week: Become an AI behavior detective! Test different AIs with the same questions and document the differences. Try to catch them hallucinating or being overconfident. Next week, we'll dive deep into bias â€“ how AI can be unfair without meaning to. Get ready to become bias detectives!

###### VISUAL
- Mission briefing style layout
- Homework checklist with detective theme
- Preview images for next lecture on bias
- QR code for homework template

###### NOTES
- Homework helps reinforce pattern recognition
- Encourage testing multiple AIs (ChatGPT o3, Claude 4 Sonnet, Gemini 2.5 Pro, DeepSeek R1-0528)
- Note: DeepSeek R1-0528 (May 2025) shows enhanced reasoning with chain-of-thought thinking
- Recent breakthrough: DeepSeek's 8B model matches much larger models through distillation
- Compare reasoning styles: DeepSeek thinks longer, shows more work, costs less than closed models
- Remind about responsible AI use

### Homework Assignment:
1. Test 3 different prompts on 2+ AI systems
2. Document one clear hallucination
3. Find one example of overconfidence
4. Bonus: Catch AI failing at a simple task
5. Bring findings to next class

### Resources:
- How AI Works (simplified): [Link to resources]
- Hallucination examples: [Link to compilation]
- AI testing playground: [Safe environment links]

---

## ARCHIVE

###### ARCHIVE

**UPDATED 2025-06-02:** Enhanced with DeepSeek-R1-0528 reasoning capabilities

**Rationale:** Added section 2.7.5 "The Reasoning Revolution" to reflect major 2024-2025 breakthrough in AI reasoning capabilities. DeepSeek-R1-0528 represents significant advancement in chain-of-thought reasoning, achieving 87.5% on AIME 2024 and enabling 8B models to match 235B models through distillation. This is directly relevant to students understanding current AI capabilities.

**Updated Content:**
- New section [seq:075] on chain-of-thought reasoning
- Updated homework notes with current model names and capabilities  
- Added cost-effectiveness discussion (DeepSeek vs closed models)
- Included recent benchmarks and technical achievements

**Archived Content:**
- Previous homework notes mentioning "Claude 4 is currently leading in coding benchmarks" â†’ replaced with more nuanced comparison
- Outdated model version references (ChatGPT 4o â†’ o3, DeepSeek R1 â†’ R1-0528)

---

*End of Lecture 2*